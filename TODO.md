# TODO для нового проекта

## 1. Генерация карты и симуляция

### 1.1. Генерация карты
- [x] Генерировать карту с размерами **width x height** (например, 100x100).
- [x] На карте должны быть:
  - [x] **Клетки с едой (TileType.FOOD)**
  - [x] **Клетки с камнями (TileType.STONE)**
  - [x] **Муравейники (TileType.COLONY)**
  - [x] **Пустые клетки (TileType.EMPTY)**

### 1.2. Статус карты
- [x] Карта должна обновляться в реальном времени:
  - [ ] **Добавление еды**  ❌ (не реализовано)
  - [x] **Исчезновение еды**
  - [x] **Перемещение агентов**
  - [x] **Учет столкновений с камнями**

### 1.3. Функции симуляции
- [x] Движение агентов (с проверкой границ и препятствий)
- [x] Проверка взаимодействия с едой и муравейником
- [x] Поддержка времени (шагов, эпох)
- [x] Жизненный цикл агентов

### 1.4. Взаимодействие агентов
- [x] Обмен знаниями о еде
- [x] Обмен состоянием (готовность сброса, местоположение и т.п.)

## 2. Структура агентов (муравьев)
- [x] Независимость и возможность общения
- [x] Состояние (x, y, несет еду и т.д.)
- [x] Память агента (hidden state)
- [x] Направление зрения (8 направлений)

## 3. Входные данные для нейросети
- [x] Зрение (one-hot по типу клетки)
- [x] Сигнал о других муравьях
- [x] Направление и флаги состояния
- [x] Память в виде вектора
- [x] Координаты (агента и муравейника)

## 4. Поведение агента (действия)
- [x] MOVE_FORWARD
- [x] MOVE_BACKWARD
- [x] PICK_FOOD
- [x] DROP_FOOD
- [x] Награды за правильные действия
- [x] Штрафы за ошибки

## 5. Нейронная сеть
- [x] Actor-Critic архитектура
- [x] LSTM или GRU
- [x] Вход: поле зрения, координаты, флаги
- [x] Выход: 6 действия 
- [ ] Exploration vs Exploitation (ε-greedy или Boltzmann)

## 6. Обучение агента
- [ ] Policy Gradient метод
- [ ] Награды и штрафы
- [ ] Эпсилон-грид стратегия
- [ ] Функция потерь (MSE или cross-entropy)

## 7. Симуляция
- [ ] Обновление карты и состояний
- [ ] Независимость агентов
- [ ] Сигналы и знания между агентами
- [ ] Отдельные параметры для каждого агента

## 8. Тестирование
- [ ] Разные конфигурации карты
- [ ] Проверка базового поведения
- [ ] Улучшение стратегии со временем

## 9. Оптимизация и улучшения
- [ ] Ускорение обучения
- [ ] Сложные формы коммуникации (сигналы, роли)

## 10. Документация
- [ ] Комментарии по алгоритмам и архитектуре
- [ ] Описание взаимодействия и наград

## 11. Дальнейшие шаги
- [ ] Обучение группы агентов
- [ ] Multi-agent RL (MARL)
- [ ] Новые уровни сложности
