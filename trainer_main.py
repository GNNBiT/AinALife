import os

import torch

from core.agent import Ant
from models.ant_net import AntNet
from models.trainer import Trainer
from core.world import World
from core.tile_types import TileType
from utils import ant_actions

from core.map_generator import generate_map, load_config

EPSILON_START = 0.5
EPSILON_END = 0.01
EPSILON_DECAY = 0.99989
epsilon = EPSILON_START

actions = [
    "MOVE_FORWARD",
    "MOVE_BACKWARD",
    "TURN_LEFT",
    "TURN_RIGHT",
    "PICK_FOOD",
    "DROP_FOOD"
]

# === 1. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ===
EPISODES = 300
SEED = 42  # –∏–ª–∏ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π seed

config = load_config("configs/map_config.yaml")  # –ø—É—Ç—å —É–∫–∞–∂–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π

# === 2. –ú–æ–¥–µ–ª—å –∏ —Ç—Ä–µ–Ω–µ—Ä ===
model = AntNet()

# üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
if os.path.exists("trained_ant_model.pth"):
    model.load_state_dict(torch.load("trained_ant_model.pth"))
    print("üîÅ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Ä–∞–Ω–µ–µ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å.")
else:
    print("üÜï –ù–æ–≤–∞—è –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –æ–±—É—á–µ–Ω–∞ —Å –Ω—É–ª—è.")

trainer = Trainer(model, lr=3e-4, gamma=0.7, use_critic=False)


for ep in range(EPISODES):
    # === 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Ä—Ç—ã –∏ –º—É—Ä–∞–≤—å—è ===
    world, colony_centers = generate_map(config)

    cx, cy = colony_centers[0]  # –ø–µ—Ä–≤—ã–π –º—É—Ä–∞–≤–µ–π–Ω–∏–∫
    ant = Ant(cx, cy, (cx, cy))

    ant.brain.model = model
    ants = [ant]
    episode_reward = 0

    while ant.is_alive():
        reward, inputs, decision = ant.act(world, ants, epsilon=epsilon)

        # –ò –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–π –∏–∑ "decision", –∫–æ—Ç–æ—Ä—ã–π —É–∂–µ –≤–µ—Ä–Ω—É–ª—Å—è –∏–∑ think:
        action_name = decision["action"]
        logits = torch.tensor(decision["logits"]).unsqueeze(0)
        value = torch.tensor([[decision["value"]]], dtype=torch.float32)

        action_dist = torch.distributions.Categorical(logits=logits)
        action_idx = action_dist.sample().item()

        # ‚úÖ –¢—É—Ç —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ:
        trainer.record_step(
            inputs,
            action_idx,
            value.item(),
            float(reward),
            logits)

        epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)
        episode_reward += reward

    trainer.train_from_trajectory_n_step(n_steps=50)

    print(f"[EPISODE {ep}] steps: {ant.steps}, energy: {ant.energy:.2f}, reward: {episode_reward:.2f}")

import json
with open("debug_data.json", "w") as f:
    json.dump(trainer.debug_log, f)
print("üìä Debug data saved to debug_data.json")


# === –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —ç–ø–∏–∑–æ–¥–∞ ===
torch.save(model.state_dict(), "trained_ant_model.pth")
print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª trained_ant_model.pth")